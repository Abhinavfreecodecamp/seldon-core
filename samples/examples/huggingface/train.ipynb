{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c57a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from alibi.explainers import AnchorText\n",
    "import spacy\n",
    "from alibi.utils import DistilbertBaseUncased\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2d0d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "pp = pipeline(\n",
    "        \"text-classification\",\n",
    "        device=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c2d5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997522234916687}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp(['hello world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4410d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from alibi.utils import spacy_model\n",
    "\n",
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e38eca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#loading the english language small model of spacy\n",
    "en = spacy.load('en_core_web_md')\n",
    "stopwords = list(en.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf003205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(x):\n",
    "    r = pp(x)\n",
    "    res = []\n",
    "    for j in r:\n",
    "        if j[\"label\"] == \"POSITIVE\":\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d75387e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_fn([\"cambridge is great\",\"Oxford is awlful\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d57d3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "language_model = DistilbertBaseUncased()\n",
    "explainer = AnchorText(\n",
    "    predictor=predict_fn,\n",
    "    sampling_strategy=\"language_model\",   # use language model to predict the masked words\n",
    "    language_model=language_model,        # language model to be used\n",
    "    filling=\"parallel\",                   # just one pass through the transformer\n",
    "    sample_proba=0.5,                     # probability of masking a word\n",
    "    frac_mask_templates=0.1,              # fraction of masking templates (smaller value -> faster, less diverse)\n",
    "    use_proba=True,                       # use words distribution when sampling (if False sample uniform)\n",
    "    top_n=20,                             # consider the fist 20 most likely words\n",
    "    temperature=1.0,                      # higher temperature implies more randomness when sampling\n",
    "    stopwords=stopwords,  # those words will not be sampled\n",
    "    batch_size_lm=32,                     # language model maximum batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d031566",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain(\"a visually exquisite but narratively opaque and emotionally vapid experience of style and mystification\", threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31f51cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Explanation(meta={\n",
       "  'name': 'AnchorText',\n",
       "  'type': ['blackbox'],\n",
       "  'explanations': ['local'],\n",
       "  'params': {\n",
       "              'seed': 0,\n",
       "              'filling': 'parallel',\n",
       "              'sample_proba': 0.5,\n",
       "              'top_n': 20,\n",
       "              'temperature': 1.0,\n",
       "              'use_proba': True,\n",
       "              'frac_mask_templates': 0.1,\n",
       "              'batch_size_lm': 32,\n",
       "              'punctuation': '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~',\n",
       "              'stopwords': ['made', '’s', 'onto', 'him', 'too', 'sixty', 'my', 'herein', 'had', 'thereby', 'your', 'front', 'to', 'and', 'somewhere', 'thereupon', 'regarding', 'latter', 'along', 'what', 'those', 'between', 'somehow', 'for', 'first', 'mostly', 'various', 'indeed', 'do', 'enough', \"n't\", 'up', \"'m\", 'back', 'others', 'whom', 'almost', 'further', '’d', 'it', 'of', 'whither', 'thereafter', 'make', '‘d', 'did', 'twenty', \"'ve\", 'cannot', 'go', 'her', 'same', 'neither', 'when', 'doing', 'noone', 'well', 'while', 'off', 'everyone', 'perhaps', 'themselves', 'moreover', 'become', 'n’t', 'nowhere', 'something', 'however', 'since', 'namely', '‘re', 'only', 'nothing', 'side', 'either', 'even', 'whereby', 'last', 'thus', 'how', 'amongst', '‘s', 'hereafter', 'afterwards', 'rather', 'whose', 'also', 'else', 'besides', 'next', 'myself', 'because', 'nine', 'about', 'can', 'using', 'within', 'am', 'could', '‘m', 'sometimes', 'are', 'whole', 'toward', 'forty', 'ours', 'no', 'ca', 'meanwhile', 'therein', 'very', 'thence', 'anyway', 'yet', 'does', 'hers', 'least', 'twelve', 'was', 'this', 'our', 'say', 'yourselves', 'much', 'elsewhere', 'should', 'their', '’m', 'anyhow', 'show', 'each', 'get', 'beside', 'whether', 'me', 'via', 'therefore', 'fifteen', 'nor', 'the', 'towards', 'once', 'from', 'wherein', 'nevertheless', 'been', 'be', 'if', 'few', 'amount', 'none', 'alone', 'latterly', 'bottom', 'otherwise', 'he', 'several', 'both', 'used', 'see', 'everything', 'fifty', 'someone', 'at', 'thru', \"'s\", \"'re\", 'beforehand', 'done', 'hundred', 'through', 'move', 'part', 'across', 'must', 'throughout', 'below', 'yours', 'may', 'into', 'every', 'n‘t', 'hence', 'many', 'there', 'where', 'becoming', 'now', 'we', 'five', 'whence', 'whoever', 'empty', 'anyone', 'were', 'behind', 'being', 'than', 'around', 'less', 'itself', 'seem', 'unless', 'whenever', 'anywhere', 'before', 'quite', 'together', 'serious', 'then', '’re', 'a', 'wherever', 'hereupon', 'take', 'would', 'above', 'always', 'formerly', 'over', 'all', 'due', 'just', 'them', 'she', 'already', 'until', 'two', 'whereas', 'who', 'whereupon', 'so', 'you', 'one', 'seemed', 'without', 'after', 'as', 'eleven', 'such', '‘ve', 'his', 'please', 'anything', 'its', 'again', 'mine', 'during', 'is', 'hereby', 'with', 'ten', 'not', 'here', 'six', \"'ll\", 'seeming', 'some', 'still', 'often', 'most', 'everywhere', 'upon', 'they', 'which', '’ve', 'under', 'us', 'any', 'ever', 'why', 'beyond', 'give', 'that', 'an', 'himself', 'became', 'except', 'in', 'other', 'against', 'on', 'but', 'has', 'top', 'though', 'i', 'out', 're', 'becomes', '’ll', 'keep', 'by', 'three', 'or', 'might', 'ourselves', 'herself', 'these', 'four', 'whatever', 'seems', 'per', 'full', 'former', 'yourself', \"'d\", 'another', '‘ll', 'nobody', 'among', 'never', 'put', 'name', 'down', 'eight', 'sometime', 'have', 'will', 'whereafter', 'third', 'although', 'own', 'call', 'really', 'more'],\n",
       "              'sample_punctuation': False}\n",
       "            ,\n",
       "  'version': '0.8.0'}\n",
       ", data={\n",
       "  'anchor': ['vapid', 'opaque'],\n",
       "  'precision': 1.0,\n",
       "  'coverage': 0.229,\n",
       "  'raw': {\n",
       "           'feature': [5, 3],\n",
       "           'mean': [0.94, 1.0],\n",
       "           'precision': [0.94, 1.0],\n",
       "           'coverage': [0.474, 0.229],\n",
       "           'examples': [{'covered_true': array(['a visually exquisite but sometimes opaque and emotionally vapid experience of style and imagery',\n",
       "       'a visually exquisite but ultimately opaque and emotionally vapid experience of style and personality',\n",
       "       'a visually exquisite but visually opaque and emotionally vapid experience of style and personality',\n",
       "       'a visually exquisite but somewhat opaque and emotionally vapid experience of style and colour',\n",
       "       'a visually exquisite but deeply opaque and emotionally vapid experience of style and detail',\n",
       "       'a visually exquisite but emotionally opaque and emotionally vapid experience of style and imagery',\n",
       "       'a visually exquisite but sometimes opaque and emotionally vapid experience of style and emotion',\n",
       "       'a visually exquisite but emotionally opaque and emotionally vapid experience of style and emotion',\n",
       "       'a visually exquisite but emotionally opaque and emotionally vapid experience of style and texture',\n",
       "       'a visually exquisite but ultimately opaque and emotionally vapid experience of style and emotion'],\n",
       "      dtype='<U109'), 'covered_false': array(['a rather exquisite but narratively pleasing and deeply vapid tale of joy and mystification',\n",
       "       'a rather exquisite but narratively touching and darkly vapid portrait of pain and mystification',\n",
       "       'a visually exquisite but narratively detailed and delicately vapid story of mystery and mystification'],\n",
       "      dtype='<U109'), 'uncovered_true': array([], dtype=float64), 'uncovered_false': array([], dtype=float64)}, {'covered_true': array(['a visually exquisite but strangely opaque and ultimately vapid experience of beauty and imagination',\n",
       "       'a visually exquisite but visually opaque and distinctly vapid experience of loneliness and beauty',\n",
       "       'a visually exquisite but visually opaque and somewhat vapid experience of mystery and deprivation',\n",
       "       'a visually exquisite but emotionally opaque and occasionally vapid experience of beauty and violence',\n",
       "       'a visually exquisite but intensely opaque and darkly vapid experience of death and death',\n",
       "       'a visually exquisite but visually opaque and potentially vapid experience of nature and violence',\n",
       "       'a visually exquisite but emotionally opaque and often vapid experience of loneliness and violence',\n",
       "       'a visually exquisite but nonetheless opaque and strangely vapid experience of love and loneliness',\n",
       "       'a visually exquisite but highly opaque and curiously vapid experience of nature and mystery',\n",
       "       'a visually exquisite but strangely opaque and visually vapid experience of love and chaos'],\n",
       "      dtype='<U105'), 'covered_false': array([], dtype='<U105'), 'uncovered_true': array([], dtype=float64), 'uncovered_false': array([], dtype=float64)}],\n",
       "           'all_precision': 0,\n",
       "           'num_preds': 1000000,\n",
       "           'success': True,\n",
       "           'positions': [9, 6],\n",
       "           'names': ['vapid', 'opaque'],\n",
       "           'instance': 'a visually exquisite but narratively opaque and emotionally vapid experience of style and mystification',\n",
       "           'instances': ['a visually exquisite but narratively opaque and emotionally vapid experience of style and mystification'],\n",
       "           'prediction': array([0])}\n",
       "         }\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5edf0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi.saving import save_explainer\n",
    "save_explainer(explainer,\"./explainer/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2713c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 12:55:49.880324: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at explainer/data/language_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnchorText(meta={\n",
       "  'name': 'AnchorText',\n",
       "  'type': ['blackbox'],\n",
       "  'explanations': ['local'],\n",
       "  'params': {\n",
       "              'seed': 0,\n",
       "              'filling': 'parallel',\n",
       "              'sample_proba': 0.5,\n",
       "              'top_n': 20,\n",
       "              'temperature': 1.0,\n",
       "              'use_proba': True,\n",
       "              'frac_mask_templates': 0.1,\n",
       "              'batch_size_lm': 32,\n",
       "              'punctuation': '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~',\n",
       "              'stopwords': ['made', '’s', 'onto', 'him', 'too', 'sixty', 'my', 'herein', 'had', 'thereby', 'your', 'front', 'to', 'and', 'somewhere', 'thereupon', 'regarding', 'latter', 'along', 'what', 'those', 'between', 'somehow', 'for', 'first', 'mostly', 'various', 'indeed', 'do', 'enough', \"n't\", 'up', \"'m\", 'back', 'others', 'whom', 'almost', 'further', '’d', 'it', 'of', 'whither', 'thereafter', 'make', '‘d', 'did', 'twenty', \"'ve\", 'cannot', 'go', 'her', 'same', 'neither', 'when', 'doing', 'noone', 'well', 'while', 'off', 'everyone', 'perhaps', 'themselves', 'moreover', 'become', 'n’t', 'nowhere', 'something', 'however', 'since', 'namely', '‘re', 'only', 'nothing', 'side', 'either', 'even', 'whereby', 'last', 'thus', 'how', 'amongst', '‘s', 'hereafter', 'afterwards', 'rather', 'whose', 'also', 'else', 'besides', 'next', 'myself', 'because', 'nine', 'about', 'can', 'using', 'within', 'am', 'could', '‘m', 'sometimes', 'are', 'whole', 'toward', 'forty', 'ours', 'no', 'ca', 'meanwhile', 'therein', 'very', 'thence', 'anyway', 'yet', 'does', 'hers', 'least', 'twelve', 'was', 'this', 'our', 'say', 'yourselves', 'much', 'elsewhere', 'should', 'their', '’m', 'anyhow', 'show', 'each', 'get', 'beside', 'whether', 'me', 'via', 'therefore', 'fifteen', 'nor', 'the', 'towards', 'once', 'from', 'wherein', 'nevertheless', 'been', 'be', 'if', 'few', 'amount', 'none', 'alone', 'latterly', 'bottom', 'otherwise', 'he', 'several', 'both', 'used', 'see', 'everything', 'fifty', 'someone', 'at', 'thru', \"'s\", \"'re\", 'beforehand', 'done', 'hundred', 'through', 'move', 'part', 'across', 'must', 'throughout', 'below', 'yours', 'may', 'into', 'every', 'n‘t', 'hence', 'many', 'there', 'where', 'becoming', 'now', 'we', 'five', 'whence', 'whoever', 'empty', 'anyone', 'were', 'behind', 'being', 'than', 'around', 'less', 'itself', 'seem', 'unless', 'whenever', 'anywhere', 'before', 'quite', 'together', 'serious', 'then', '’re', 'a', 'wherever', 'hereupon', 'take', 'would', 'above', 'always', 'formerly', 'over', 'all', 'due', 'just', 'them', 'she', 'already', 'until', 'two', 'whereas', 'who', 'whereupon', 'so', 'you', 'one', 'seemed', 'without', 'after', 'as', 'eleven', 'such', '‘ve', 'his', 'please', 'anything', 'its', 'again', 'mine', 'during', 'is', 'hereby', 'with', 'ten', 'not', 'here', 'six', \"'ll\", 'seeming', 'some', 'still', 'often', 'most', 'everywhere', 'upon', 'they', 'which', '’ve', 'under', 'us', 'any', 'ever', 'why', 'beyond', 'give', 'that', 'an', 'himself', 'became', 'except', 'in', 'other', 'against', 'on', 'but', 'has', 'top', 'though', 'i', 'out', 're', 'becomes', '’ll', 'keep', 'by', 'three', 'or', 'might', 'ourselves', 'herself', 'these', 'four', 'whatever', 'seems', 'per', 'full', 'former', 'yourself', \"'d\", 'another', '‘ll', 'nobody', 'among', 'never', 'put', 'name', 'down', 'eight', 'sometime', 'have', 'will', 'whereafter', 'third', 'although', 'own', 'call', 'really', 'more'],\n",
       "              'sample_punctuation': False}\n",
       "            ,\n",
       "  'version': '0.8.0'}\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from alibi.saving import load_explainer\n",
    "load_explainer(path=\"./explainer/data\", predictor=predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96744d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
